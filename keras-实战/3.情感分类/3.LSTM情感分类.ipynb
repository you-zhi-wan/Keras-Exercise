{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import jieba # pip install jieba\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Input, Flatten, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读入数据\n",
    "neg=pd.read_excel('data/neg.xls',header=None)\n",
    "pos=pd.read_excel('data/pos.xls',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10428\n",
      "10677\n"
     ]
    }
   ],
   "source": [
    "#合并语料\n",
    "pn = pd.concat([pos,neg],ignore_index=True) \n",
    "#计算语料数目\n",
    "neglen = len(neg)\n",
    "print(neglen)\n",
    "poslen = len(pos) \n",
    "print(poslen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\DELL\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.603 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "#定义分词函数\n",
    "cw = lambda x: list(jieba.cut(x))\n",
    "pn['words'] = pn[0].apply(cw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>做父母一定要有刘墉这样的心态，不断地学习，不断地进步，不断地给自己补充新鲜血液，让自己保持一...</td>\n",
       "      <td>[做, 父母, 一定, 要, 有, 刘墉, 这样, 的, 心态, ，, 不断, 地, 学习,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>作者真有英国人严谨的风格，提出观点、进行论述论证，尽管本人对物理学了解不深，但是仍然能感受到...</td>\n",
       "      <td>[作者, 真有, 英国人, 严谨, 的, 风格, ，, 提出, 观点, 、, 进行, 论述,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>作者长篇大论借用详细报告数据处理工作和计算结果支持其新观点。为什么荷兰曾经县有欧洲最高的生产...</td>\n",
       "      <td>[作者, 长篇大论, 借用, 详细, 报告, 数据处理, 工作, 和, 计算结果, 支持, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>作者在战几时之前用了＂拥抱＂令人叫绝．日本如果没有战败，就有会有美军的占领，没胡官僚主义的延...</td>\n",
       "      <td>[作者, 在, 战, 几时, 之前, 用, 了, ＂, 拥抱, ＂, 令人, 叫绝, ．, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>作者在少年时即喜阅读，能看出他精读了无数经典，因而他有一个庞大的内心世界。他的作品最难能可贵...</td>\n",
       "      <td>[作者, 在, 少年, 时即, 喜, 阅读, ，, 能, 看出, 他, 精读, 了, 无数,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21100</th>\n",
       "      <td>安装我自己花了500多，美的够黑心的，真的是烦心，安装的售后叼的要死！差评！！！！！</td>\n",
       "      <td>[安装, 我, 自己, 花, 了, 500, 多, ，, 美的, 够, 黑心, 的, ，, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21101</th>\n",
       "      <td>东西不错，售后太差，安装一个热水器400块钱，像话吗？钱给完了发现被黑了！</td>\n",
       "      <td>[东西, 不错, ，, 售后, 太, 差, ，, 安装, 一个, 热水器, 400, 块钱,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21102</th>\n",
       "      <td>碰到最差的、最骗人的卖家，好吧，我倒霉！这个卖家太不厚道了，显示所在地上海，我买这个热水器选...</td>\n",
       "      <td>[碰到, 最差, 的, 、, 最, 骗人, 的, 卖家, ，, 好, 吧, ，, 我, 倒霉...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21103</th>\n",
       "      <td>宝贝不错，物流也不错，售后差，</td>\n",
       "      <td>[宝贝, 不错, ，, 物流, 也, 不错, ，, 售后, 差, ，]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21104</th>\n",
       "      <td>美的售后太垃圾，其他售后都是两小时回电话，美的是24小时，结果超过市区不到五公里问我收五十，...</td>\n",
       "      <td>[美的, 售后, 太, 垃圾, ，, 其他, 售后, 都, 是, 两, 小时, 回, 电话,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21105 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       0  \\\n",
       "0      做父母一定要有刘墉这样的心态，不断地学习，不断地进步，不断地给自己补充新鲜血液，让自己保持一...   \n",
       "1      作者真有英国人严谨的风格，提出观点、进行论述论证，尽管本人对物理学了解不深，但是仍然能感受到...   \n",
       "2      作者长篇大论借用详细报告数据处理工作和计算结果支持其新观点。为什么荷兰曾经县有欧洲最高的生产...   \n",
       "3      作者在战几时之前用了＂拥抱＂令人叫绝．日本如果没有战败，就有会有美军的占领，没胡官僚主义的延...   \n",
       "4      作者在少年时即喜阅读，能看出他精读了无数经典，因而他有一个庞大的内心世界。他的作品最难能可贵...   \n",
       "...                                                  ...   \n",
       "21100         安装我自己花了500多，美的够黑心的，真的是烦心，安装的售后叼的要死！差评！！！！！   \n",
       "21101              东西不错，售后太差，安装一个热水器400块钱，像话吗？钱给完了发现被黑了！   \n",
       "21102  碰到最差的、最骗人的卖家，好吧，我倒霉！这个卖家太不厚道了，显示所在地上海，我买这个热水器选...   \n",
       "21103                                    宝贝不错，物流也不错，售后差，   \n",
       "21104  美的售后太垃圾，其他售后都是两小时回电话，美的是24小时，结果超过市区不到五公里问我收五十，...   \n",
       "\n",
       "                                                   words  \n",
       "0      [做, 父母, 一定, 要, 有, 刘墉, 这样, 的, 心态, ，, 不断, 地, 学习,...  \n",
       "1      [作者, 真有, 英国人, 严谨, 的, 风格, ，, 提出, 观点, 、, 进行, 论述,...  \n",
       "2      [作者, 长篇大论, 借用, 详细, 报告, 数据处理, 工作, 和, 计算结果, 支持, ...  \n",
       "3      [作者, 在, 战, 几时, 之前, 用, 了, ＂, 拥抱, ＂, 令人, 叫绝, ．, ...  \n",
       "4      [作者, 在, 少年, 时即, 喜, 阅读, ，, 能, 看出, 他, 精读, 了, 无数,...  \n",
       "...                                                  ...  \n",
       "21100  [安装, 我, 自己, 花, 了, 500, 多, ，, 美的, 够, 黑心, 的, ，, ...  \n",
       "21101  [东西, 不错, ，, 售后, 太, 差, ，, 安装, 一个, 热水器, 400, 块钱,...  \n",
       "21102  [碰到, 最差, 的, 、, 最, 骗人, 的, 卖家, ，, 好, 吧, ，, 我, 倒霉...  \n",
       "21103                [宝贝, 不错, ，, 物流, 也, 不错, ，, 售后, 差, ，]  \n",
       "21104  [美的, 售后, 太, 垃圾, ，, 其他, 售后, 都, 是, 两, 小时, 回, 电话,...  \n",
       "\n",
       "[21105 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1804"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 一行数据最多的词汇数\n",
    "max_document_length = max([len(x) for x in pn['words']])\n",
    "max_document_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置一个评论最多1000个词\n",
    "max_document_length = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'宝贝 不错 ， 物流 也 不错 ， 售后 差 ，'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [' '.join(x) for x in pn['words']]\n",
    "# 查看一条评论\n",
    "texts[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实例化分词器，设置字典中最大词汇数为30000\n",
    "tokenizer = Tokenizer(num_words=30000)\n",
    "# 传入我们的训练数据，建立词典\n",
    "tokenizer.fit_on_texts(texts) \n",
    "# 把词转换为编号，词的编号根据词频设定，频率越大，编号越小\n",
    "sequences = tokenizer.texts_to_sequences(texts) \n",
    "# 把序列设定为1000的长度，超过1000的部分舍弃，不到1000则补0\n",
    "sequences = pad_sequences(sequences, maxlen=1000)  \n",
    "sequences = np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 词对应编号的字典\n",
    "dict_text = tokenizer.word_index\n",
    "dict_text['也']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "       1318,   24,    1, 1482,    9,   24,    1,  909,  156,    1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义标签\n",
    "positive_labels = [[0, 1] for _ in range(poslen)]\n",
    "negative_labels = [[1, 0] for _ in range(neglen)]\n",
    "y = np.concatenate([positive_labels, negative_labels], 0)\n",
    "\n",
    "# 打乱数据\n",
    "np.random.seed(10)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = sequences[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]\n",
    "\n",
    "# 数据集切分为两部分\n",
    "test_sample_index = -1 * int(0.1 * float(len(y)))\n",
    "x_train, x_test = x_shuffled[:test_sample_index], x_shuffled[test_sample_index:]\n",
    "y_train, y_test = y_shuffled[:test_sample_index], y_shuffled[test_sample_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.layers.wrappers import Bidirectional\n",
    "from tensorflow.python.keras.layers.recurrent import LSTM\n",
    "from tensorflow.python.keras.layers.recurrent import GRU\n",
    "# 定义函数式模型\n",
    "# 模型输入\n",
    "sequence_input = Input(shape=(1000,))\n",
    "# Embedding层，30000表示30000个词，每个词对应的向量为128维，序列长度为1000\n",
    "embedding_layer = Embedding(30000,\n",
    "                            128,\n",
    "                            input_length=1000)\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "\n",
    "# 方法一，LSTM的输出为(batch, 10)\n",
    "lstm1 = LSTM(10, dropout=0.2, recurrent_dropout=0.2)(embedded_sequences)\n",
    "lstm1 = Dense(16, activation='relu')(lstm1)\n",
    "lstm1 = Dropout(0.5)(lstm1)\n",
    "\n",
    "# 方法二，LSTM的输出为(batch, 1000, 10)\n",
    "# lstm1 = LSTM(10,return_sequences=True,dropout=0.2, recurrent_dropout=0.2)(embedded_sequences)\n",
    "# lstm1 = Flatten()(lstm1)\n",
    "# lstm1 = Dense(16, activation='relu')(lstm1)\n",
    "# lstm1 = Dropout(0.5)(lstm1)\n",
    "\n",
    "# 方法三\n",
    "# 可以结合采用多种RNN模型\n",
    "# lstm2 = Bidirectional(LSTM(10,return_sequences=True,dropout=0.2, recurrent_dropout=0.2))(embedded_sequences)\n",
    "# lstm2 = Flatten()(lstm2)\n",
    "# lstm2 = Dense(16, activation='relu')(lstm2)\n",
    "# lstm2 = Dropout(0.5)(lstm2)\n",
    "\n",
    "# lstm3 = GRU(10,return_sequences=True,dropout=0.2, recurrent_dropout=0.2)(embedded_sequences)\n",
    "# lstm3 = Flatten()(lstm3)\n",
    "# lstm3 = Dense(16, activation='relu')(lstm3)\n",
    "# lstm3 = Dropout(0.5)(lstm3)\n",
    "# merge = concatenate([lstm1, lstm2, lstm3], axis=1)\n",
    "# merge=Dense(16,activation='relu')(merge)\n",
    "# merge=Dropout(0.5)(merge)\n",
    "\n",
    "# 输出层\n",
    "preds = Dense(2, activation='softmax')(lstm1)\n",
    "# 定义模型\n",
    "model = Model(sequence_input, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "149/149 [==============================] - 285s 2s/step - loss: 0.5449 - acc: 0.7446 - val_loss: 0.2974 - val_acc: 0.8915\n",
      "Epoch 2/5\n",
      "149/149 [==============================] - 343s 2s/step - loss: 0.2876 - acc: 0.9007 - val_loss: 0.2514 - val_acc: 0.9100\n",
      "Epoch 3/5\n",
      "149/149 [==============================] - 337s 2s/step - loss: 0.1799 - acc: 0.9489 - val_loss: 0.2380 - val_acc: 0.9199\n",
      "Epoch 4/5\n",
      "149/149 [==============================] - 344s 2s/step - loss: 0.1243 - acc: 0.9702 - val_loss: 0.2587 - val_acc: 0.9204\n",
      "Epoch 5/5\n",
      "149/149 [==============================] - 347s 2s/step - loss: 0.0885 - acc: 0.9794 - val_loss: 0.2793 - val_acc: 0.9161\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x294e8880ee0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练模型\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=128,\n",
    "          epochs=5,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测\n",
    "def predict(text):\n",
    "    # 对句子分词\n",
    "    cw = list(jieba.cut(text)) \n",
    "    word_id = []\n",
    "    # 把词转换为编号\n",
    "    for word in cw:\n",
    "        try:\n",
    "            temp = dict_text[word]\n",
    "            word_id.append(temp)\n",
    "        except:\n",
    "            word_id.append(0)\n",
    "    word_id = np.array(word_id)\n",
    "    word_id = word_id[np.newaxis,:]\n",
    "    sequences = pad_sequences(word_id, maxlen=1000, padding='post')  \n",
    "    result = np.argmax(model.predict(sequences))\n",
    "    if(result==1):\n",
    "        print(\"positive comment\")\n",
    "    else:\n",
    "        print(\"negative comment\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive comment\n"
     ]
    }
   ],
   "source": [
    "predict(\"东西质量不错，下次还会再来买\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
